{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n"
      ],
      "metadata": {
        "id": "F9oNJ5grll3c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "MPJwDTM9vPkT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIeIX80SYwnE",
        "outputId": "0e85c401-becc-42c6-836b-aaa69d6771bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Downloading sentence_transformers-3.4.0-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "Hfq1SP0qb02d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6bSvLoWb5o5",
        "outputId": "2b5ff58b-d69c-4909-b1b2-d095f1a40a84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL3gCvH3X3V2",
        "outputId": "35ac45c4-9f87-4163-e1df-ed082f80db00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "698mZ-iuXb1C"
      },
      "outputs": [],
      "source": [
        "def load_corpus(file_path):\n",
        "    corpus = []\n",
        "    doc_names = []\n",
        "    if file_path.endswith(\".txt\"):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            corpus.append(f.read())\n",
        "            doc_names.append(os.path.basename(file_path))\n",
        "    elif file_path.endswith(\".pdf\"):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\".join(page.extract_text() for page in reader.pages)\n",
        "            corpus.append(text)\n",
        "            doc_names.append(os.path.basename(file_path))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a .txt or .pdf file.\")\n",
        "    return corpus, doc_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingSearch:\n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.doc_embeddings = None\n",
        "\n",
        "    def fit(self, corpus):\n",
        "      self.corpus = []\n",
        "      self.doc_embeddings = []\n",
        "      for document in corpus:\n",
        "          chunks = [document[i:i+512] for i in range(0, len(document), 512)]  # Split into chunks of 512 tokens\n",
        "          self.corpus.extend(chunks)\n",
        "          chunk_embeddings = self.model.encode(chunks, convert_to_tensor=True)\n",
        "          self.doc_embeddings.append(chunk_embeddings)\n",
        "      self.doc_embeddings = torch.cat(self.doc_embeddings)\n",
        "\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "      query = query[:512]\n",
        "      query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "\n",
        "      similarities = cosine_similarity(query_embedding.unsqueeze(0), self.doc_embeddings)[0]\n",
        "      top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "      return [(self.corpus[idx], similarities[idx]) for idx in top_indices]\n"
      ],
      "metadata": {
        "id": "56kx5uCiZQfo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "class EmbeddingSearch:\n",
        "    def fit(self, corpus):\n",
        "        # Dummy implementation for embedding search setup\n",
        "        self.corpus = corpus\n",
        "\n",
        "    def search(self, query, top_k=3):\n",
        "        # Dummy implementation for searching relevant documents\n",
        "        results = [(doc, 1.0 - i * 0.1) for i, doc in enumerate(self.corpus[:top_k])]\n",
        "        return results\n",
        "\n",
        "def load_corpus(file_path):\n",
        "    if file_path.endswith(\".txt\"):\n",
        "        with open(file_path, \"r\") as file:\n",
        "            text = file.read()\n",
        "        return text.split(\"\\n\\n\"), [\"Document\"]\n",
        "    elif file_path.endswith(\".pdf\"):\n",
        "        try:\n",
        "            from PyPDF2 import PdfReader\n",
        "            reader = PdfReader(file_path)\n",
        "            text = \"\\n\".join(page.extract_text() for page in reader.pages)\n",
        "            return text.split(\"\\n\\n\"), [\"Document\"]\n",
        "        except ImportError:\n",
        "            raise Exception(\"Install PyPDF2 to process PDF files.\")\n",
        "    else:\n",
        "        raise Exception(\"Unsupported file format. Use .txt or .pdf files.\")\n",
        "\n",
        "def fine_tune_transformer(model_name):\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "def generate_response(gpt_pipeline, query, relevant_docs):\n",
        "    # Check if relevant_docs is valid\n",
        "    if not relevant_docs or not isinstance(relevant_docs[0], (list, tuple)):\n",
        "        print(\"No relevant documents found or incorrect structure.\")\n",
        "        return \"I couldn't find relevant information to answer your query.\"\n",
        "\n",
        "    # Extract the context\n",
        "    context = relevant_docs[0][0]\n",
        "    if not isinstance(context, str):\n",
        "        context = str(context)\n",
        "\n",
        "    # Truncate the context to a reasonable length\n",
        "    context = context[:512]\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    try:\n",
        "        response = gpt_pipeline(prompt, max_new_tokens=100, num_return_sequences=1)\n",
        "        if isinstance(response, list) and \"generated_text\" in response[0]:\n",
        "            return response[0][\"generated_text\"]\n",
        "        else:\n",
        "            return \"I couldn't generate a valid response. Please try again.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return \"An error occurred while generating the response.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # File path for the document\n",
        "    file_path = input(\"Enter the path to your document (txt or pdf): \").strip()\n",
        "\n",
        "    # Load corpus\n",
        "    print(\"Loading document...\")\n",
        "    try:\n",
        "        corpus, doc_names = load_corpus(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # Initialize embedding-based search\n",
        "    print(\"Generating embeddings...\")\n",
        "    embed_search = EmbeddingSearch()\n",
        "    embed_search.fit(corpus)\n",
        "\n",
        "    # Initialize GPT model pipeline\n",
        "    print(\"Loading GPT model...\")\n",
        "    tokenizer, model = fine_tune_transformer(\"gpt2\")\n",
        "    gpt_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nEnter your search query (or 'exit' to quit):\")\n",
        "        query = input().strip()\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Search for relevant documents\n",
        "        print(\"Searching for relevant content...\")\n",
        "        relevant_docs = embed_search.search(query, top_k=3)\n",
        "\n",
        "        print(\"\\nRelevant Content:\")\n",
        "        for i, (doc, score) in enumerate(relevant_docs):\n",
        "            print(f\"{i+1}. (Score: {score:.2f})\\n{doc[:200]}...\")\n",
        "\n",
        "        # Generate a response using GPT\n",
        "        print(\"\\nGenerating response using GPT...\")\n",
        "        response = generate_response(gpt_pipeline, query, relevant_docs)\n",
        "        print(f\"\\nResponse:\\n{response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvkwwih5Z35C",
        "outputId": "2f66e6ce-2e4d-48f5-cb80-9811e8651a32"
      },
      "execution_count": 40,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading document...\n",
            "Generating embeddings...\n",
            "Loading GPT model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter your search query (or 'exit' to quit):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for relevant content...\n",
            "\n",
            "Relevant Content:\n",
            "1. (Score: 1.00)\n",
            "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/344717762\n",
            "Machine Learning Algorithms -A Review\n",
            "Technic al R eport    in  Internat...\n",
            "\n",
            "Generating response using GPT...\n",
            "\n",
            "Response:\n",
            "Context: See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/344717762\n",
            "Machine Learning Algorithms -A Review\n",
            "Technic al R eport    in  International Journal of Scienc e and R esearch (IJSR)  · Januar y 2019\n",
            "DOI: 10.21275/ ART20203995\n",
            "CITATIONS\n",
            "1,333READS\n",
            "410,174\n",
            "1 author:\n",
            "Batt a Mahesh\n",
            "Independent R esearcher\n",
            "5 PUBLICA TIONS    1,341  CITATIONS    \n",
            "SEE PROFILE\n",
            "All c ontent f ollo wing this p age was uplo aded b y Batt a Mahesh  on 17 Oct ober 2020.\n",
            "Th\n",
            "\n",
            "Question: What is Machine Learning?\n",
            "\n",
            "Answer: Machine learning refers to what I shall called the computer process to learn new skills, not the process of memorizing which concepts and information be deduced from them. This is the way human thought is thought to develop from that of the brain to understand the world around us at a rate faster then any form of perception or visualization.\n",
            "\n",
            "The only way to learn is to use information from the world around us and our ability to process it, not from external sources, without any other external influence.\n",
            "\n",
            "\n",
            "Enter your search query (or 'exit' to quit):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for relevant content...\n",
            "\n",
            "Relevant Content:\n",
            "1. (Score: 1.00)\n",
            "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/344717762\n",
            "Machine Learning Algorithms -A Review\n",
            "Technic al R eport    in  Internat...\n",
            "\n",
            "Generating response using GPT...\n",
            "\n",
            "Response:\n",
            "Context: See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/344717762\n",
            "Machine Learning Algorithms -A Review\n",
            "Technic al R eport    in  International Journal of Scienc e and R esearch (IJSR)  · Januar y 2019\n",
            "DOI: 10.21275/ ART20203995\n",
            "CITATIONS\n",
            "1,333READS\n",
            "410,174\n",
            "1 author:\n",
            "Batt a Mahesh\n",
            "Independent R esearcher\n",
            "5 PUBLICA TIONS    1,341  CITATIONS    \n",
            "SEE PROFILE\n",
            "All c ontent f ollo wing this p age was uplo aded b y Batt a Mahesh  on 17 Oct ober 2020.\n",
            "Th\n",
            "\n",
            "Question: what the document is talking?\n",
            "\n",
            "Answer: The I N O T N C C R O E M E L E T A S O R D S C R O N A '\n",
            "\n",
            "What is I C C R e r i n g?\n",
            "\n",
            "Answer: What is i c l i n g?\n",
            "\n",
            "Question: What is I r e p p e n p e n s a t d i m d h i s u e a t t u s?\n",
            "\n",
            "Answer: What is a l e n g\n",
            "\n",
            "Enter your search query (or 'exit' to quit):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for relevant content...\n",
            "\n",
            "Relevant Content:\n",
            "1. (Score: 1.00)\n",
            "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/344717762\n",
            "Machine Learning Algorithms -A Review\n",
            "Technic al R eport    in  Internat...\n",
            "\n",
            "Generating response using GPT...\n",
            "\n",
            "Response:\n",
            "Context: See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/344717762\n",
            "Machine Learning Algorithms -A Review\n",
            "Technic al R eport    in  International Journal of Scienc e and R esearch (IJSR)  · Januar y 2019\n",
            "DOI: 10.21275/ ART20203995\n",
            "CITATIONS\n",
            "1,333READS\n",
            "410,174\n",
            "1 author:\n",
            "Batt a Mahesh\n",
            "Independent R esearcher\n",
            "5 PUBLICA TIONS    1,341  CITATIONS    \n",
            "SEE PROFILE\n",
            "All c ontent f ollo wing this p age was uplo aded b y Batt a Mahesh  on 17 Oct ober 2020.\n",
            "Th\n",
            "\n",
            "Question: quit\n",
            "\n",
            "Answer:\n",
            "\n",
            "1.1 We are making use of a type of machine learning system which is being used by students for their studies in R and computer science. We use a technique known as a model t. We t he current research focuses upon the \"bundling\" of data in and out of a deep web. We develop a tool for the analysis of data from a wide range of data domains, e. g. database, web traffic, web analytics, search engines and search engine analytics.\n",
            "\n",
            "Enter your search query (or 'exit' to quit):\n",
            "exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gpt4all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS7fkNbSxbbE",
        "outputId": "dae0eeb8-a70f-4f64-bff0-c701d9a17585"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2024.12.14)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.8.2\n"
          ]
        }
      ]
    }
  ]
}